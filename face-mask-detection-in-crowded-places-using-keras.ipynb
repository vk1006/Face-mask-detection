{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How to create a Deep Learning face mask classifier for COVID-19 in public spaces","metadata":{}},{"cell_type":"markdown","source":"### Step 1: Importing the necessary Python libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np  # linear algebra\nimport cv2 # opencv\nimport matplotlib.pyplot as plt # image plotting\n# keras\nfrom keras import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.vgg19 import preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"execution":{"iopub.status.busy":"2022-07-10T09:39:26.402853Z","iopub.execute_input":"2022-07-10T09:39:26.403165Z","iopub.status.idle":"2022-07-10T09:39:31.855724Z","shell.execute_reply.started":"2022-07-10T09:39:26.403081Z","shell.execute_reply":"2022-07-10T09:39:31.854829Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: Getting the data","metadata":{}},{"cell_type":"code","source":"# Load train and test set\ntrain_dir = \"../input/face-mask-12k-images-dataset/Face Mask Dataset/Train\"\ntest_dir = \"../input/face-mask-12k-images-dataset/Face Mask Dataset/Test\"\nval_dir = \"../input/face-mask-12k-images-dataset/Face Mask Dataset/Validation\"","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-10T09:39:31.861665Z","iopub.execute_input":"2022-07-10T09:39:31.863920Z","iopub.status.idle":"2022-07-10T09:39:31.870046Z","shell.execute_reply.started":"2022-07-10T09:39:31.863879Z","shell.execute_reply":"2022-07-10T09:39:31.869315Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Step 3: Reading a sample image and performing face detection","metadata":{}},{"cell_type":"code","source":"# Read a sample image\nimg = cv2.imread(\"../input/face-mask-detection/images/maksssksksss352.png\")\n\n# Keep a copy of coloured image\norig_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)  # colored output image\n\n# Convert image to grayscale\nimg = cv2.cvtColor(img, cv2.IMREAD_GRAYSCALE)\n\n# loading haarcascade_frontalface_default.xml\nface_detection_model = cv2.CascadeClassifier(\"../input/haar-cascades-for-face-detection/haarcascade_frontalface_default.xml\")\n\n# detect faces in the given image\nreturn_faces = face_detection_model.detectMultiScale(\n    img, scaleFactor=1.08, minNeighbors=4\n)  # returns a list of (x,y,w,h) tuples\n\n# plotting the returned values\nfor (x, y, w, h) in return_faces:\n    cv2.rectangle(orig_img, (x, y), (x + w, y + h), (0, 0, 255), 1)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(orig_img)  # display the image","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-10T09:39:31.875422Z","iopub.execute_input":"2022-07-10T09:39:31.878367Z","iopub.status.idle":"2022-07-10T09:39:32.930614Z","shell.execute_reply.started":"2022-07-10T09:39:31.878327Z","shell.execute_reply":"2022-07-10T09:39:32.927979Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Step 4: Data preprocessing for building the mask detection Keras model","metadata":{}},{"cell_type":"code","source":"# Data preprocessing\n# Train data\ndatagenerator = ImageDataGenerator(\n    rescale=1.0 / 255, horizontal_flip=True, zoom_range=0.2, shear_range=0.2\n)\ntrain_generator = datagenerator.flow_from_directory(\n    directory=train_dir, target_size=(128, 128), class_mode=\"categorical\", batch_size=32\n)\n\n# Validation data\nval_generator = datagenerator.flow_from_directory(\n    directory=val_dir, target_size=(128, 128), class_mode=\"categorical\", batch_size=32\n)\n\n# Test data\ntest_generator = datagenerator.flow_from_directory(\n    directory=val_dir, target_size=(128, 128), class_mode=\"categorical\", batch_size=32\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-10T09:39:32.931773Z","iopub.execute_input":"2022-07-10T09:39:32.931982Z","iopub.status.idle":"2022-07-10T09:39:42.342459Z","shell.execute_reply.started":"2022-07-10T09:39:32.931955Z","shell.execute_reply":"2022-07-10T09:39:42.341710Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Step 5: Create the mask detection transfer learning model using Keras","metadata":{}},{"cell_type":"code","source":"# Initializing the VGG19 model\nvgg19_model = VGG19(weights=\"imagenet\", include_top=False, input_shape=(128, 128, 3))\n\nfor layer in vgg19_model.layers:\n    layer.trainable = False\n\n# Initialize a sequential model\nmodel = Sequential()\nmodel.add(vgg19_model)\nmodel.add(Flatten())\nmodel.add(Dense(2, activation=\"sigmoid\"))\nmodel.summary()\n\n# Compiling the model\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=\"accuracy\")","metadata":{"_kg_hide-output":true,"tags":[],"execution":{"iopub.status.busy":"2022-07-10T09:39:42.344557Z","iopub.execute_input":"2022-07-10T09:39:42.344987Z","iopub.status.idle":"2022-07-10T09:39:46.032928Z","shell.execute_reply.started":"2022-07-10T09:39:42.344948Z","shell.execute_reply":"2022-07-10T09:39:46.032187Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Step 6: Train the model","metadata":{"tags":[]}},{"cell_type":"code","source":"# Fit the model on train data along with validation data\nmodel_history = model.fit_generator(\n    generator=train_generator,\n    steps_per_epoch=len(train_generator) // 32,\n    epochs=20,\n    validation_data=val_generator,\n    validation_steps=len(val_generator) // 32,\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-10T09:39:46.034375Z","iopub.execute_input":"2022-07-10T09:39:46.034633Z","iopub.status.idle":"2022-07-10T09:40:54.487431Z","shell.execute_reply.started":"2022-07-10T09:39:46.034597Z","shell.execute_reply":"2022-07-10T09:40:54.486636Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Step 7: Evaluate the model performance on test set","metadata":{"tags":[]}},{"cell_type":"code","source":"# Evaluate model performance on test data\nmodel_loss, model_acc = model.evaluate(test_generator)\nprint(\"Model has a loss of %.2f and accuracy %.2f%%\" % (model_loss, model_acc*100))","metadata":{"execution":{"iopub.status.busy":"2022-07-10T09:40:54.489043Z","iopub.execute_input":"2022-07-10T09:40:54.489321Z","iopub.status.idle":"2022-07-10T09:41:02.255167Z","shell.execute_reply.started":"2022-07-10T09:40:54.489286Z","shell.execute_reply":"2022-07-10T09:41:02.254432Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Step 8: Save the model\n\nWe can also choose to save the trained model as a h5 file for future use.","metadata":{}},{"cell_type":"code","source":"model.save('data/saved_model.h5')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-10T09:41:02.256474Z","iopub.execute_input":"2022-07-10T09:41:02.257099Z","iopub.status.idle":"2022-07-10T09:41:02.428276Z","shell.execute_reply.started":"2022-07-10T09:41:02.257056Z","shell.execute_reply":"2022-07-10T09:41:02.427529Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Step 7: Test the model on the sample image","metadata":{}},{"cell_type":"markdown","source":"We will now test the trained model on our use case for detecting faces and masks for a group of people. We take the detected face crops of the faces detected in the image and then predict the mask or no mask using the model trained.","metadata":{}},{"cell_type":"code","source":"# label for mask detection\nmask_det_label = {0: \"Mask\", 1: \"No Mask\"}\nmask_det_label_colour = {0: (0, 255, 0), 1: (255, 0, 0)}\npad_y = 1  # padding for result text\n\nmain_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)  # colored output image\n\n# For detected faces in the image\nfor i in range(len(return_faces)):\n    (x, y, w, h) = return_faces[i]\n    cropped_face = main_img[y : y + h, x : x + w]\n    cropped_face = cv2.resize(cropped_face, (128, 128))\n    cropped_face = np.reshape(cropped_face, [1, 128, 128, 3]) / 255.0\n    mask_result = model.predict(cropped_face)  # make model prediction\n    print_label = mask_det_label[mask_result.argmax()] # get mask/no mask based on prediction\n    label_colour = mask_det_label_colour[mask_result.argmax()] # green for mask, red for no mask\n\n    # Print result\n    (t_w, t_h), _ = cv2.getTextSize(\n        print_label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1\n    )  # getting the text size\n    \n    cv2.rectangle(\n        main_img,\n        (x, y + pad_y),\n        (x + t_w, y - t_h - pad_y - 6),\n        label_colour,\n        -1,\n    )  # draw rectangle\n\n    cv2.putText(\n        main_img,\n        print_label,\n        (x, y - 6),\n        cv2.FONT_HERSHEY_DUPLEX,\n        0.4,\n        (255, 255, 255), # white\n        1,\n    )  # print text\n\n    cv2.rectangle(\n        main_img,\n        (x, y),\n        (x + w, y + h),\n        label_colour,\n        1,\n    )  # draw bounding box on face\n\nplt.figure(figsize=(10, 10))\nplt.imshow(main_img)  # display image","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-10T09:41:02.429626Z","iopub.execute_input":"2022-07-10T09:41:02.429901Z","iopub.status.idle":"2022-07-10T09:41:03.168448Z","shell.execute_reply.started":"2022-07-10T09:41:02.429863Z","shell.execute_reply":"2022-07-10T09:41:03.167750Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model is correctly detecting faces and classifying them as mask and no mask.\n\n-- THANK YOU -- ","metadata":{}}]}